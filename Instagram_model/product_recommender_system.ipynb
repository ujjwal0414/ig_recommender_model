{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8853b1-c39a-48a5-b409-7df7907bcb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (1.32.0)\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (4.25.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.19.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ujwal\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ujwal\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Downloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/8.7 MB 1.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.5/8.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 6.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/8.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.7/8.7 MB 6.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.8/8.7 MB 6.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.8/8.7 MB 6.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/8.7 MB 5.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/8.7 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.8/8.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.8/8.7 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.4/8.7 MB 5.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.9/8.7 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.4/8.7 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.6/8.7 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.1/8.7 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.7/8.7 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.2/8.7 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.9/8.7 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.5/8.7 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: streamlit\n",
      "  Attempting uninstall: streamlit\n",
      "    Found existing installation: streamlit 1.32.0\n",
      "    Uninstalling streamlit-1.32.0:\n",
      "      Successfully uninstalled streamlit-1.32.0\n",
      "Successfully installed streamlit-1.38.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c3f348f-37c9-48c2-a5e0-516cfc76c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-recommenders\n",
    "!pip install -q --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8efdf409-e481-4e76-935e-494d0def5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep openpyxl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d54588-bdab-489e-bb5b-35294738ee86",
   "metadata": {},
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e49e8d12-e20f-409d-9a09-37256433bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fde729-d1d7-464a-ad78-9392d893c7f7",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a5b46",
   "metadata": {},
   "source": [
    "Firstly, we have to prepare the dataset and convert it the tensor style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3adb3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset():\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df_ = df[:100000]\n",
    "        self.users_df = pd.DataFrame\n",
    "\n",
    "    def feature_selection(self):\n",
    "        # The first 100,000 records were selected because the original dataset was too large to train\n",
    "        self.df_ = self.df_[self.df_['Quantity']>=1].dropna().reset_index(drop = True)\n",
    "\n",
    "        # Select  essential features \n",
    "        self.users_df = self.df_[['Customer ID', 'Description']]\n",
    "        help_ = self.users_df[['Description']].drop_duplicates()\n",
    "        help_['product_id'] = [i+1 for i in range(help_.shape[0])]  \n",
    "        self.users_df = self.users_df.merge(help_, on='Description'\n",
    "                            ).drop('Description', axis=1\n",
    "                            ).rename(columns = {'Description' : 'product_id', 'Customer ID': 'user_id'})\n",
    "        return self.users_df\n",
    "        \n",
    "    def create_tensor_dataset(self):\n",
    "        # Convert the dataframe to tensor format\n",
    "        self.users_df.user_id = self.users_df.user_id.apply(lambda x: str(int(x)))\n",
    "        self.users_df.product_id = self.users_df.product_id.apply(lambda x: str(x))\n",
    "\n",
    "        self.users_df = self.users_df.sample(frac=1).reset_index(drop=True)\n",
    "        self.products_df = self.users_df[['product_id']]   \n",
    "\n",
    "        self.users_dataset = tf.data.Dataset.from_tensor_slices(dict(self.users_df))\n",
    "        self.products_dataset = tf.data.Dataset.from_tensor_slices(dict(self.products_df))\n",
    "        return self.users_dataset, self.products_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555015ce-255f-42a5-b899-cc01f08039a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Implement Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d5d033",
   "metadata": {},
   "source": [
    "After converting the dataset, it's time to implement the recommender system on the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d611f26b-33fa-43ba-bc39-83db15f3cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_manipulation:\n",
    "    \n",
    "    def __init__(self, users, products):\n",
    "        self.users = users\n",
    "        self.products = products\n",
    "        \n",
    "    # keep useful elements\n",
    "    def keep_useful_elements(self):\n",
    "        self.users = self.users.map(lambda x: {\n",
    "                         'product_id' : x['product_id'],\n",
    "                         'user_id' : x['user_id'],\n",
    "                    })\n",
    "        self.products = self.products.map(lambda x: x['product_id'])\n",
    "        return self.users, self.products \n",
    "    \n",
    "    # Train test split\n",
    "    def train_test_generator(self, train_range=80_000, all_range=100_000):\n",
    "        tf.random.set_seed(42)\n",
    "        shuffled = self.users.shuffle(all_range, seed=42, reshuffle_each_iteration=False)\n",
    "        train = shuffled.take(train_range)\n",
    "        test = shuffled.skip(train_range).take(all_range - train_range)\n",
    "        return train, test\n",
    "    \n",
    "    # Create a list of unique products and users\n",
    "    def pass_unique(self):\n",
    "        product_ids = self.products.batch(1_000)\n",
    "        user_ids = self.users.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
    "\n",
    "        unique_product_ids = np.unique(np.concatenate(list(product_ids)))\n",
    "        unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n",
    "        return unique_product_ids, unique_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dbc3f6f-7ad1-4ec1-8583-87d893bfc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelAndLoss:\n",
    "    \n",
    "    def __init__(self, unique_product_ids, unique_user_ids, products):\n",
    "        self.unique_product_ids = unique_product_ids\n",
    "        self.unique_user_ids = unique_user_ids\n",
    "        self.products = products\n",
    "\n",
    "    # Here, we're going to use Keras preprocessing layers to first convert user ids to integers, and then convert those\n",
    "    # to user embeddings via an Embedding layer.\n",
    "    def implement_model(self, embedding_dimension = 32):\n",
    "        user_model = tf.keras.Sequential([\n",
    "          tf.keras.layers.StringLookup(\n",
    "              vocabulary=self.unique_user_ids, mask_token=None),\n",
    "          # Add an additional embedding to account for unknown tokens.\n",
    "          tf.keras.layers.Embedding(len(self.unique_user_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "        \n",
    "        # the candidate tower\n",
    "        self.product_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.StringLookup(\n",
    "          vocabulary=self.unique_product_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(len(self.unique_product_ids) + 1, embedding_dimension)\n",
    "        ])\n",
    "        return user_model, self.product_model\n",
    "    \n",
    "    def metrics_loss(self, batch_size = 128):\n",
    "        metrics = tfrs.metrics.FactorizedTopK(\n",
    "          candidates= self.products.batch(batch_size).map(self.product_model)\n",
    "        )\n",
    "        task = tfrs.tasks.Retrieval(\n",
    "            metrics=metrics)\n",
    "        return task\n",
    "        # Precompute product embeddings\n",
    "        # product_embeddings = self.products.batch(batch_size).map(lambda x: self.product_model(x))\n",
    "\n",
    "        # # Use the precomputed embeddings for the FactorizedTopK metric\n",
    "        # metrics = tfrs.metrics.FactorizedTopK(\n",
    "        #     candidates=tf.data.Dataset.from_tensor_slices(self.unique_product_ids).batch(batch_size).map(self.product_model)\n",
    "        # )\n",
    "        \n",
    "        # # Define the retrieval task\n",
    "        # task = tfrs.tasks.Retrieval(\n",
    "        #     metrics=metrics\n",
    "        # )\n",
    "        \n",
    "        # return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48616cd9-37eb-48c6-b6f2-3ee48fa6d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class modelAndLoss:\n",
    "    \n",
    "#     def __init__(self, unique_product_ids, unique_user_ids, products):\n",
    "#         self.unique_product_ids = unique_product_ids\n",
    "#         self.unique_user_ids = unique_user_ids\n",
    "#         self.products = products\n",
    "\n",
    "#     # Create the user and product models\n",
    "#     def implement_model(self, embedding_dimension=32):\n",
    "#         # User model\n",
    "#         self.user_model = tf.keras.Sequential([\n",
    "#             tf.keras.layers.StringLookup(vocabulary=self.unique_user_ids, mask_token=None),\n",
    "#             tf.keras.layers.Embedding(len(self.unique_user_ids) + 1, embedding_dimension)\n",
    "#         ])\n",
    "        \n",
    "#         # Product model (candidate tower)\n",
    "#         self.product_model = tf.keras.Sequential([\n",
    "#             tf.keras.layers.StringLookup(vocabulary=self.unique_product_ids, mask_token=None),\n",
    "#             tf.keras.layers.Embedding(len(self.unique_product_ids) + 1, embedding_dimension)\n",
    "#         ])\n",
    "        \n",
    "#         return self.user_model, self.product_model\n",
    "\n",
    "#     # Manual Top-K retrieval function\n",
    "#     def manual_top_k(self, user_embeddings, product_embeddings, k=10):\n",
    "#         # Compute the dot product (similarity) between user and product embeddings\n",
    "#         similarity_scores = tf.matmul(user_embeddings, product_embeddings, transpose_b=True)\n",
    "        \n",
    "#         # Get top-k product indices based on similarity scores\n",
    "#         top_k_values, top_k_indices = tf.nn.top_k(similarity_scores, k=k)\n",
    "        \n",
    "#         return top_k_indices, top_k_values\n",
    "\n",
    "#     # Custom loss and metrics function\n",
    "#     def metrics_loss(self, batch_size=128, k=10):\n",
    "#         # Precompute product embeddings for all products in the dataset\n",
    "#         product_embeddings = tf.concat([self.product_model(tf.constant([prod_id])) for prod_id in self.unique_product_ids], axis=0)\n",
    "        \n",
    "#         # Example user embedding (you would replace this with the actual user input)\n",
    "#         user_embeddings = self.user_model(tf.constant(self.unique_user_ids))  # Assuming all users are considered\n",
    "        \n",
    "#         # Compute the top-k products for each user\n",
    "#         top_k_indices, top_k_values = self.manual_top_k(user_embeddings, product_embeddings, k=k)\n",
    "\n",
    "#         # Define your retrieval task (if necessary, you can compute a custom loss here)\n",
    "#         task = {\n",
    "#             \"top_k_indices\": top_k_indices,\n",
    "#             \"top_k_values\": top_k_values\n",
    "#         }\n",
    "\n",
    "#         return task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "246ff110-713e-4332-b00b-79508af8c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class userProductModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, user_model, product_model):\n",
    "        super().__init__()\n",
    "        self.product_model: tf.keras.Model = product_model\n",
    "        self.user_model: tf.keras.Model = user_model\n",
    "        self.task: tf.keras.layers.Layer = task\n",
    "\n",
    "    # Now it's time to implement the full model\n",
    "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "        # We pick out the user features and pass them into the user model.\n",
    "        self.user_embeddings = self.user_model(features[\"user_id\"])\n",
    "        # And pick out the product features and pass them into the product model,\n",
    "        # getting embeddings back.\n",
    "        self.positive_product_embeddings = self.product_model(features[\"product_id\"])\n",
    "\n",
    "        # The task computes loss and the metrics.\n",
    "        return self.task(self.user_embeddings, self.positive_product_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b93ff5e-8c90-444c-88e6-168ea3e01264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and evaluating\n",
    "class fitAndEvaluateModel:\n",
    "\n",
    "    # As the final stage, we create, compile, fit, and evaluate our model\n",
    "    \n",
    "    def __init__(self, user_model, product_model, train, test):\n",
    "        self.user_model = user_model\n",
    "        self.product_model = product_model\n",
    "        self.model = None\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        \n",
    "    def create_model(self):\n",
    "        self.model = userProductModel(self.user_model, self.product_model) \n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "    \n",
    "    def fit_model(self):\n",
    "        cached_train = train.shuffle(200_000).batch(8192).cache()\n",
    "        self.cached_test = test.batch(4096).cache()\n",
    "        self.model.fit(cached_train, epochs=10)\n",
    "        return self.model\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        self.model.evaluate(self.cached_test, return_dict=True)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65afa3-2265-4d3f-91a8-dc7611b2e842",
   "metadata": {},
   "source": [
    "# Recommend products to users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148cc12e",
   "metadata": {},
   "source": [
    "After fitting and evaluating the model, it's time to make suggestions to users.\n",
    "In this example, we considered user numnber \"15865\" as our sample, however, it could be any user number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ee1089b-b884-4080-afd8-150cdf883929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, products, product_model):\n",
    "    # Create a model that takes in raw query features, and\n",
    "    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "    # recommends products out of the entire products dataset.\n",
    "    index.index_from_dataset(\n",
    "      tf.data.Dataset.zip((products.batch(100), products.batch(100).map(model.product_model)))\n",
    ")\n",
    "\n",
    "    _, ids = index(tf.constant(['15865']))\n",
    "    return ids, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ab40766-efbe-4fac-b3dd-b77013bade80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_predictions(model, products, product_model):\n",
    "#     # Create a model that takes in raw query features, and\n",
    "#     index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "    \n",
    "#     # Ensure that the product_model processes batched products correctly\n",
    "#     product_embeddings = products.map(lambda x: (x, model.product_model(x)))\n",
    "    \n",
    "#     # Index the product embeddings into the BruteForce model\n",
    "#     index.index_from_dataset(product_embeddings.batch(100))\n",
    "\n",
    "#     # Test the prediction with a constant user id\n",
    "#     _, ids = index(tf.constant(['15865']))\n",
    "    \n",
    "#     return ids, index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33fb03fd-577f-4e70-8ac5-b22d7c25e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86318ba-1457-497b-a44b-6732a3312708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d556a4-5b31-4570-9fd1-6cf6af2b0601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee995adf-8712-405b-8158-f11548d05ac2",
   "metadata": {},
   "source": [
    "# Build, compile, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17e34503-853f-4545-99c3-3919485ff7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = './Dataset/online_retail_II.xlsx'\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "dataset = PrepareDataset(df)\n",
    "users_df = dataset.feature_selection()\n",
    "users, products = dataset.create_tensor_dataset()\n",
    "\n",
    "data = data_manipulation(users, products)\n",
    "users, products = data.keep_useful_elements()\n",
    "train, test = data.train_test_generator()\n",
    "unique_product_ids, unique_user_ids = data.pass_unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b2eed35-117e-475b-a152-7f67c0309468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TrackedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcreate_model()\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile_model()\n\u001b[1;32m----> 9\u001b[0m model_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 22\u001b[0m, in \u001b[0;36mfitAndEvaluateModel.fit_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m cached_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m200_000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m8192\u001b[39m)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m4096\u001b[39m)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n",
      "File \u001b[1;32m~\\Twin_tower_model\\twin_model\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Twin_tower_model\\twin_model\\Lib\\site-packages\\tensorflow_recommenders\\models\\base.py:68\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom train step using the `compute_loss` method.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 68\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m   \u001b[38;5;66;03m# Handle regularization losses as well.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m   regularization_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "Cell \u001b[1;32mIn[42], line 18\u001b[0m, in \u001b[0;36muserProductModel.compute_loss\u001b[1;34m(self, features, training)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive_product_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproduct_model(features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# The task computes loss and the metrics.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive_product_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TrackedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "pre_model = modelAndLoss(unique_product_ids, unique_user_ids, products)\n",
    "user_model, product_model = pre_model.implement_model()\n",
    "# task = pre_model.metrics_loss()\n",
    "# product_model\n",
    "\n",
    "model = fitAndEvaluateModel(user_model, product_model, train, test)\n",
    "model.create_model()\n",
    "model.compile_model()\n",
    "model_ = model.fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c710b-e34c-42cd-8ab8-a7fc6e0869f1",
   "metadata": {},
   "source": [
    "# Torch model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbc4841b-03c5-4951-85b6-bbad2df19f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9895404577255249\n",
      "Epoch 2/10, Loss: 0.6394259035587311\n",
      "Epoch 3/10, Loss: 0.377838134765625\n",
      "Epoch 4/10, Loss: 0.20817303657531738\n",
      "Epoch 5/10, Loss: 0.11031857132911682\n",
      "Epoch 6/10, Loss: 0.06631645560264587\n",
      "Epoch 7/10, Loss: 0.04353860020637512\n",
      "Epoch 8/10, Loss: 0.03541871905326843\n",
      "Epoch 9/10, Loss: 0.027033984661102295\n",
      "Epoch 10/10, Loss: 0.021920382976531982\n",
      "Top 3 recommended items for user1: ['item3' 'item1' 'item5']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Mock dataset\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids):\n",
    "        self.user_ids = user_ids\n",
    "        self.item_ids = item_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx]\n",
    "\n",
    "# Define the User and Item Embedding Towers\n",
    "class TwoTowerRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
    "        super(TwoTowerRecommender, self).__init__()\n",
    "        # User embedding tower\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.user_fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "        # Item embedding tower\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.item_fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Forward pass for users and items\n",
    "        user_embeds = self.user_fc(self.user_embedding(user_ids))\n",
    "        item_embeds = self.item_fc(self.item_embedding(item_ids))\n",
    "        \n",
    "        return user_embeds, item_embeds\n",
    "\n",
    "    def get_user_embedding(self, user_id):\n",
    "        # Get user embedding for a single user\n",
    "        user_embedding = self.user_fc(self.user_embedding(user_id))\n",
    "        return user_embedding\n",
    "\n",
    "    def get_all_item_embeddings(self):\n",
    "        # Get embeddings for all items\n",
    "        all_items = torch.arange(self.item_embedding.num_embeddings).to(device)\n",
    "        item_embeddings = self.item_fc(self.item_embedding(all_items))\n",
    "        return item_embeddings\n",
    "\n",
    "    def recommend(self, user_id, top_n=5):\n",
    "        # Get the user embedding for the specific user\n",
    "        user_embedding = self.get_user_embedding(user_id)\n",
    "\n",
    "        # Get all item embeddings\n",
    "        item_embeddings = self.get_all_item_embeddings()\n",
    "\n",
    "        # Compute similarity (dot product) between the user and all items\n",
    "        scores = torch.matmul(item_embeddings, user_embedding.T).squeeze()\n",
    "\n",
    "        # Get top-N item indices based on the scores\n",
    "        top_n_scores, top_n_indices = torch.topk(scores, top_n)\n",
    "\n",
    "        return top_n_indices, top_n_scores\n",
    "\n",
    "# Loss function (contrastive loss using cosine similarity)\n",
    "def cosine_similarity_loss(user_embeds, item_embeds):\n",
    "    similarity = torch.cosine_similarity(user_embeds, item_embeds)\n",
    "    loss = 1 - similarity.mean()\n",
    "    return loss\n",
    "\n",
    "# Example training loop\n",
    "def train_model(model, data_loader, epochs=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for user_ids, item_ids in data_loader:\n",
    "            user_ids, item_ids = user_ids.to(device), item_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            user_embeds, item_embeds = model(user_ids, item_ids)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = cosine_similarity_loss(user_embeds, item_embeds)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(data_loader)}\")\n",
    "\n",
    "# Mock Data\n",
    "users = ['user1', 'user2', 'user3', 'user4']\n",
    "items = ['item1', 'item2', 'item3', 'item4', 'item5']\n",
    "\n",
    "# Encoding user and item IDs to integers\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "user_ids = user_encoder.fit_transform(users)\n",
    "item_ids = item_encoder.fit_transform(items)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = InteractionDataset(torch.tensor(user_ids), torch.tensor(item_ids))\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Define model parameters\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_items = len(item_encoder.classes_)\n",
    "embedding_dim = 32\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = TwoTowerRecommender(num_users, num_items, embedding_dim).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, data_loader, epochs=10)\n",
    "\n",
    "# Example inference: Recommend top-N items for a specific user\n",
    "user_id = torch.tensor([user_encoder.transform(['user1'])[0]]).to(device)\n",
    "top_n_items, top_n_scores = model.recommend(user_id, top_n=3)\n",
    "\n",
    "# Decode item indices to item labels\n",
    "recommended_items = item_encoder.inverse_transform(top_n_items.cpu().numpy())\n",
    "\n",
    "print(f\"Top 3 recommended items for user1: {recommended_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c386ff-2eeb-4385-ac50-b785f2928354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twin_model_kernel",
   "language": "python",
   "name": "twin_model_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
